{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6928d21-163c-4761-acce-0bd133e8416c",
   "metadata": {},
   "source": [
    "# Prompt SCI open-source LLMs through an API\n",
    "In this notebook, you will get started with prompting through an API and using templates (parameterized) prompts. You will prompt open-source LLMs set up for use by Pitt's School of Computing and Information. \n",
    "\n",
    "Making API calls to these models is not currently available to do from the CRCD. Hopefully that will change in the next few weeks! For now, you'll have to make API calls from your own local machines.\n",
    "\n",
    "**You will need to connect to the Pitt VPN to make API calls to SCI LLMs, even if you're on campus. Here are instructions for doing that through the GlobalProtect app: https://services.pitt.edu/TDClient/33/Portal/KB/ArticleDet?ID=293**\n",
    "\n",
    "We will use the `openai` Python package to make API calls to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0daee5-bee3-4136-9b5e-57d28966491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c83c5-3ed0-49dd-93d9-c3e13298934a",
   "metadata": {},
   "source": [
    "# Create client and set API key\n",
    "You'll need to check the class Canvas announcement for the API key for the Pitt SCI open-source LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a2ac5-6a0d-4b01-bd14-a33212014b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sci_api_key = ''\n",
    "client = openai.OpenAI( \n",
    "    api_key=class_sci_api_key, \n",
    "    base_url=\"https://ol.sci.pitt.edu\" # LiteLLM Proxy is OpenAI compatible, Read More: https://docs.litellm.ai/docs/proxy/user_keys \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36cb05b-0306-4ea3-a8e6-c5bb4a49045d",
   "metadata": {},
   "source": [
    "# Start prompting!\n",
    "SCI Tech has set up 3 open-source models on their servers available for us to use:\n",
    "* `gemma3` (gemma3:27b)\n",
    "* `llama3.1` (llama3.1:70b)\n",
    "* `deepseek-r1` (deepseek-r1:70b)\n",
    "\n",
    "Feel free to try any of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0afe3-15a4-4008-aa58-6ea99bbf229a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = 'Did the construction of Boulevard of the Allies in Pittsburgh displace communities?'\n",
    "# prompt = 'Counting the cars on the New Jersey Turnpike, ...'\n",
    "\n",
    "def call_llm(prompt, model='gemma3'):\n",
    "    \"\"\" Make an API call to Pitt SCI LLM \n",
    "        Args:\n",
    "            model: {gemma3, llama3.1, deepseek-r1}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create( \n",
    "        model = model, # model to send to the proxy \n",
    "        messages = [{ \n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "        }] \n",
    "    ) \n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text\n",
    "\n",
    "call_llm(prompt, model='gemma3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acb0ac-d6c8-4612-819f-873c8153a73b",
   "metadata": {},
   "source": [
    "Try other models. Do they give similar responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29095766-f80c-4e6c-befb-94f23527324d",
   "metadata": {},
   "source": [
    "# Try automated prompting through templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d27186-30e8-4750-bf11-ca3e5955f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = [\n",
    "    'inflammable',\n",
    "    'car',\n",
    "    'monkey',\n",
    "    'actually'\n",
    "] # this could be loaded from a dataset, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d5b3a-f405-40e2-afe6-9c9f08d656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = 'What is another word for {input}?'\n",
    "\n",
    "for word in input_words:\n",
    "    prompt = prompt_template.replace('{input}', word)\n",
    "    print(f'Prompt: {prompt}')\n",
    "    response = call_llm(prompt)\n",
    "    print(response)\n",
    "    print('***************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7f9a7-b452-4d03-9f9d-b9bfb6c520c9",
   "metadata": {},
   "source": [
    "How might you modify your prompt to get a more helpful answer, such as a more concise answer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
