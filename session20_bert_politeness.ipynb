{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6928d21-163c-4761-acce-0bd133e8416c",
   "metadata": {},
   "source": [
    "# Finetune a BERT model for politeness classification\n",
    "BERT models are often used for text classification tasks. BERT produces an output embedding for every input token; the special `[CLS]` token is the output embedding usually used for text classification. Due to self-attention, this embedding contains information from all the tokens in the sentence so can be used as a representation for the whole sentence.\n",
    "\n",
    "In this notebook, you will finetune a DistilBERT model, a small BERT model, for the task of politeness classification, determining whether a sentence is polite or not.\n",
    "\n",
    "This notebook is best run on a GPU\n",
    "\n",
    "Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0daee5-bee3-4136-9b5e-57d28966491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c83c5-3ed0-49dd-93d9-c3e13298934a",
   "metadata": {},
   "source": [
    "# Load politeness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a2ac5-6a0d-4b01-bd14-a33212014b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/politeness.csv')\n",
    "# Rename `polite` column to `label`\n",
    "data = data.rename(columns={'polite': 'label'})\n",
    "data['text'] = data['text'].str.lower() # lowercase\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36cb05b-0306-4ea3-a8e6-c5bb4a49045d",
   "metadata": {},
   "source": [
    "# Finetune DistilBERT for politeness classification\n",
    "Using Hugging Face ðŸ¤— tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edaa53-6fc4-4e83-a9d4-1cbcf7e8441f",
   "metadata": {},
   "source": [
    "## Get dataset into the Hugging Face input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34465b-ea37-4557-9eb7-9373ba0d4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(data).train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851ef7b-9b48-4e1a-afae-d4b3eae6fb95",
   "metadata": {},
   "source": [
    "## Set up tokenization and initialize the model\n",
    "This will load subword tokenization models that have been pretrained on data to recognize lots of subwords and apply them to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdffd1-f57a-452c-b5f8-1a17c8a79f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True) # truncates to DistilBERT's maximum input length\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # pads to the correct length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04241e8-98f7-4702-beb3-8019f394bbcc",
   "metadata": {},
   "source": [
    "Now we'll initialize a pretrained DistilBERT Hugging Face model for 'sequence classification', which is text classification. We will set up necessary hyperparameters for training (finetuning) the model with the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db06536-f25c-46c1-85b6-b8ff077d26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label = {0: \"NOT POLITE\", 1: \"NOT POLITE\"}\n",
    "label2id = {\"NOT POLITE\": 0, \"POLITE\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a4ecd-732a-4bbe-a851-aba03fbb1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up evaluation\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16580d7d-30c1-4f2a-b726-b1718f83a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training hyperparameters and initialize model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"politeness_classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5287c7-5d33-4b09-bc08-a2e8ba766e39",
   "metadata": {},
   "source": [
    "## Finetune (train) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096eef4d-4809-409e-8636-bde24c5a4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e78231-1289-4632-9271-2e119669deea",
   "metadata": {},
   "source": [
    "## Evaluate performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59d68c-79e0-4b23-9ab9-7a345a8e836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(tokenized_dataset['test'])\n",
    "pd.DataFrame(results, index=['Fine-tuned DistilBERT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fa90d-603f-4d42-a15e-dc1d2329234f",
   "metadata": {},
   "source": [
    "This is a hard task and our DistilBERT model has room for improvement. \n",
    "\n",
    "Feel free to play around with the training hyperparameters, retrain, and see if you can get better accuracy. You can also try other pretrained models such as `distilroberta-base`, `bert-base-uncased`, or `roberta-base`. Just substitute the names of the pretrained tokenizers and models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
