{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb4Yr8KhRhbe"
   },
   "source": [
    "# Homework 3 part 1: Large language model (LLM) prompting\n",
    "\n",
    "## Learning objectives\n",
    "After completing this assignment, students will be able to:     \n",
    "* Prompt LLMs programmatically with templates (parameterized)\n",
    "* Prompt LLMs with zero-shot and few-shot prompting\n",
    "* Engineer and test different prompts\n",
    "\n",
    "## Overview\n",
    "In this part of the assignment, you will explore different prompting techniques for open-source LLMs. You will fill in a Jupyter notebook hosted on the Pitt CRCD to run your code. The default model is set to `gemma3` (gemma3:27b), but feel free to explore the other models, too. They are accessed by specifying `llama3.1` (llam3.1:70b) and `deepseek-r1` (deepseek-r1:70b) in the `run_llm` function.\n",
    "\n",
    "## Deliverables\n",
    "1. Your code: the Jupyter notebook you modified from the template for part 1. Submit:\n",
    "    * your .ipynb file\n",
    "    * a **.html export of your notebook**. To get a .html version, click File > Save and Export Notebook As... > HTML from within JupyterLab. \n",
    "2. A PDF report with answers to questions provided in the template notebook. Please name your report `hw3_{your pitt email id}.pdf`. No need to include @pitt.edu, just use the email ID before that part. For example: `report_mmyoder_hw3.pdf`. **Please make only one PDF report, containing answers to part 1 and part 2.** Make sure to include the following information:\n",
    "    * answers to all the numbered questions below (prompts and their outputs, along with which model gave the output)\n",
    "    * any additional resources, references, or web pages you've consulted\n",
    "    * any person with whom you've discussed the assignment and describe the nature of your discussions\n",
    "    * any generative AI tool used, and how it was used\n",
    "    * any unresolved issues or problems\n",
    "\n",
    "Please submit all of this material on Canvas. We will grade your report and may look over your code.\n",
    "\n",
    "## Recommended Readings\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf). Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, ...others. ArXiV 2020.\n",
    "- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf). Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. ACM Computing Surveys 2021.\n",
    "- [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf). Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, ...others. ArXiV 2020.\n",
    "- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf). Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, Denny Zhou. NeurIPS 2022.\n",
    "\n",
    "## Acknowledgments\n",
    "This assignment is based on a homework assignment designed by Mark Yatskar and provided by Lorraine Li.\n",
    "\n",
    "**To get started, start filling in this Jupyter notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHrdyiQdBt2K"
   },
   "source": [
    "## Setup 1: Dataset / Package\n",
    "**Run the following cells and enter the class API Key for Pitt SCI LLMs!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNBeJLVcSusU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from time import sleep\n",
    "from datasets import load_dataset\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNBeJLVcSusU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMDB_DATASET = load_dataset(\"imdb\", split='train').shuffle(42)[0:200]\n",
    "IMDB_DATASET_X = IMDB_DATASET['text']\n",
    "IMDB_DATASET_Y = IMDB_DATASET['label']\n",
    "del IMDB_DATASET\n",
    "\n",
    "## TODO - Start\n",
    "LLM_API_KEY = \"\" # the class API key for Pitt SCI LLMs here\n",
    "## TODO - End\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def run_llm(prompt, model='gemma3', return_first_line=False):\n",
    "    cache_key = (prompt, model, return_first_line)\n",
    "    if cache_key in cache:\n",
    "        return cache[cache_key]\n",
    "    \n",
    "    client = OpenAI( \n",
    "        api_key=LLM_API_KEY,\n",
    "        base_url=\"https://ol.sci.pitt.edu\")\n",
    "        \n",
    "    # Send prompt to API\n",
    "    for i in range(0,60,6):\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                temperature=0,\n",
    "                max_tokens=300,\n",
    "                top_p=1,\n",
    "            )\n",
    "            response = dict(response)['choices'][0]\n",
    "            response = dict(response)['text'].strip()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            sleep(i)\n",
    "\n",
    "    # Parse the response\n",
    "    if return_first_line:\n",
    "        final_response = response.split('.')[0]+'.'\n",
    "        if '\\n' in final_response:\n",
    "            final_response = response.split('\\n')[0]\n",
    "    else:\n",
    "        final_response = response\n",
    "\n",
    "    # Cache and return the response\n",
    "    cache[cache_key] = final_response\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_34IstJ9bTOt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grade_llm_starts_with_answer(prompt, input, answer, **kwargs):\n",
    "    model_output = run_llm(prompt.replace(\"{input}\", input), **kwargs).strip().lower()\n",
    "    answer = answer.lower()\n",
    "    return model_output.startswith(answer)\n",
    "\n",
    "def grade_llm_contains_answer(prompt, first_line_only, input, answer, **kwargs):\n",
    "    model_output = run_llm(prompt.replace(\"{input}\", input), return_first_line=first_line_only, **kwargs).strip().lower()\n",
    "    answer = answer.lower()\n",
    "    return answer in model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKvfbQtibDXM"
   },
   "source": [
    "## Setup 2 Define Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m02ctSwIbOKK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_capital_of_country(prompt):\n",
    "    correct = sum([\n",
    "        grade_llm_contains_answer(prompt, True, \"Canada\", \"Ottawa\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"India\", \"New Delhi\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"Turkey\", \"Ankara\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"China\", \"Beijing\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"Japan\", \"Tokyo\")\n",
    "    ])\n",
    "\n",
    "    if correct >= 3:\n",
    "        return (3, 3)\n",
    "    else:\n",
    "        return (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Uxep2iJc2lO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_director_of_movie(prompt):\n",
    "    correct = sum([\n",
    "        grade_llm_contains_answer(prompt, True, \"Toy Story\", \"John Lasseter\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"Pulp Fiction\", \"Quentin Tarantino\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"Jurassic Park\", \"Steven Spielberg\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"Star Wars: Episode IV – A New Hope\", \"George Lucas\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"The Dark Knight\", \"Christopher Nolan\")\n",
    "    ])\n",
    "    if correct >= 3:\n",
    "        return (3, 3)\n",
    "    else:\n",
    "        return (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_Nh2PuKdECe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_synonyms_of_word(prompt, first_line_only):\n",
    "    correct = sum([\n",
    "        grade_llm_contains_answer(prompt, first_line_only, \"car\", \"vehicle\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only, \"cold\", \"frigid\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only,\"mad\", \"angry\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only,\"desk\", \"table\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only,\"gift\", \"present\")\n",
    "    ])\n",
    "    if correct >= 3:\n",
    "        return (3, 3)\n",
    "    else:\n",
    "        return (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jp4fwp6dEFO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_ingredients_of_food(prompt, first_line_only):\n",
    "    correct = sum([\n",
    "        grade_llm_contains_answer(prompt, first_line_only, \"tiramisu\", \"coffee\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only, \"pesto\", \"leaves\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only, \"samosa\", \"masala\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only, \"hummus\", \"chickpeas\"),\n",
    "        grade_llm_contains_answer(prompt, first_line_only, \"macaroon\", \"coconut\")\n",
    "    ])\n",
    "    if correct >= 3:\n",
    "        return (3, 3)\n",
    "    else:\n",
    "        return (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhWHQtOddEIO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_quotee_of_quote(prompt):\n",
    "    correct = sum([\n",
    "        grade_llm_starts_with_answer(prompt, '\"If you can\\'t handle me at my worst, then you sure as hell don\\'t deserve me at my best.\"', \"Marilyn Monroe\"),\n",
    "        grade_llm_contains_answer(prompt, True, '\"The only thing we have to fear is fear itself.\"', \"Roosevelt\"),\n",
    "        grade_llm_starts_with_answer(prompt, '\"Genius is one percent inspiration and ninety-nine percent perspiration.\"', \"Thomas Edison\"),\n",
    "        grade_llm_starts_with_answer(prompt, '\"Nothing is certain except for death and taxes.\"', \"Benjamin Franklin\"),\n",
    "        grade_llm_starts_with_answer(prompt, '\"Life is like riding a bicycle. To keep your balance, you must keep moving.\"', \"Albert Einstein\")\n",
    "    ])\n",
    "    if correct >= 3:\n",
    "        if any([q_word in prompt.lower() for q_word in [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]]):\n",
    "            return (1, 3)\n",
    "        if \"?\" in prompt:\n",
    "            return (1, 3)\n",
    "        return (3, 3)\n",
    "    else:\n",
    "        return (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up9uYcrcdEK9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_korean_to_english(prompt):\n",
    "    correct = sum([\n",
    "        grade_llm_contains_answer(prompt, True, \"책상\", \"desk\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"책\", \"book\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"창문\", \"window\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"나무\", \"tree\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"트럭\", \"truck\")\n",
    "    ])\n",
    "    if correct >= 5:\n",
    "        return (5, 5)\n",
    "    else:\n",
    "        return (0, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CW8jHzodENj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_to_jeopardy_answer(prompt):\n",
    "    correct = sum([\n",
    "        grade_llm_contains_answer(prompt, True, \"Hawaii\", \"is Hawaii?\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"trees\", \"What are trees?\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"The Empire State Building\", \"What is the Empire State Building?\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"Neil Armstrong\", \"Who is Neil Armstrong?\"),\n",
    "        grade_llm_contains_answer(prompt, True, \"John Legend\", \"Who is John Legend?\")\n",
    "    ])\n",
    "    if correct >= 5:\n",
    "        return (5, 5)\n",
    "    else:\n",
    "        return (0, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfdC0RotdEQC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_english_to_spanish(prompt):\n",
    "    correct = sum([\n",
    "        grade_llm_starts_with_answer(prompt, \"desk\", \"escritorio\", instruction_tuned=True),\n",
    "        grade_llm_starts_with_answer(prompt, \"book\", \"libro\", instruction_tuned=True),\n",
    "        grade_llm_starts_with_answer(prompt, \"window\", \"ventana\", instruction_tuned=True),\n",
    "        grade_llm_starts_with_answer(prompt, \"bed\", \"cama\", instruction_tuned=True),\n",
    "        grade_llm_starts_with_answer(prompt, \"bread\", \"pan\", instruction_tuned=True)\n",
    "    ])\n",
    "    if correct >= 5:\n",
    "        return (5, 5)\n",
    "    else:\n",
    "        return (0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4X9ztQJwmyp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_message(score, max_score):\n",
    "    if score == max_score:\n",
    "        print('Correct! You earned {}/{} points. You are a star!'.format(score, max_score))\n",
    "    else:\n",
    "        print(\"You missed some points, try to check what's wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGlvLsC9B-gI"
   },
   "source": [
    "# Section 1: Exploring Prompting\n",
    "**Background:** Prompting is a way to guide a language model, which is ultimately just a model that predicts the most likely next sequence of words, to complete some arbitrary task you want it to complete. We'll walk through a few examples and then you'll try creating your own prompts.\n",
    "\n",
    "A language model will \"complete\" (just like autocomplete) your prompt with what words are most likely to come next. We demonstrate this is the case by showing how LLMs completes movie quotes, when giving it the beginning of the quote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcILHclcou0L",
    "outputId": "69e99d1a-1b2a-49d5-a5e0-ab6fdc787438",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(run_llm(\"Life is like a box of chocolates,\", return_first_line=True))\n",
    "print(run_llm(\"With great power,\", return_first_line=True))\n",
    "print(run_llm(\"The name's Bond.\", return_first_line=True))\n",
    "print(run_llm(\"Houston, we\", return_first_line=True))\n",
    "print(run_llm(\"I've a feeling we're not in\", return_first_line=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJKLr2kjatM3"
   },
   "source": [
    "Now imagine we give a prompt like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-XKt2tpZzi7",
    "outputId": "f2a57f77-da96-4c79-a533-61490ffcc2b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(run_llm(\"Question: Who was the first president of the United States? Answer:\", return_first_line=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q41x7bVXbLY-"
   },
   "source": [
    "By posing a question and writing \"Answer:\" at the end, we make it such that the most likely next sequence of words is the answer to the question! This is the key to large language models being able to perform arbitrary tasks, even though they are only trained to predict the next word.\n",
    "\n",
    "We can parameterize this prompt and make it reusable for different questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeHF3-OPZ601",
    "outputId": "d15bbff6-51c3-432c-a2fa-686dffa06247",
    "tags": []
   },
   "outputs": [],
   "source": [
    "QA_PROMPT = \"Question: {input} Answer:\"\n",
    "print(run_llm(QA_PROMPT.replace(\"{input}\", \"What company did Steve Jobs found?\"), return_first_line=True))\n",
    "print(run_llm(QA_PROMPT.replace(\"{input}\", \"What's the movie with Tom Cruise about fighter jets?\"), return_first_line=True))\n",
    "print(run_llm(QA_PROMPT.replace(\"{input}\", \"Are tomatoes a fruit or a vegetable?\"), return_first_line=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWLiOfB_fB-T"
   },
   "source": [
    "Now that you've seen a few examples it's time for you to come up with a few of your own prompts! Make sure you parameterize them with `{input}` before sending the prompt.\n",
    "\n",
    "Note: These models are not easy to control. Therefore, it's okay if your prompt does not always get the answer right or also spews extra text along with the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUnnuRJfbXf5"
   },
   "source": [
    "- **Problem 1.1:** Write a prompt that returns the capital of country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVUR92UPbSkn"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "CAPITAL_OF_COUNTRY_PROMPT = \"\"\n",
    "\n",
    "# Grader - DO NOT CHANGE\n",
    "your_score, max_score = test_capital_of_country(CAPITAL_OF_COUNTRY_PROMPT)\n",
    "print_message(your_score, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-GAJhyvbido"
   },
   "source": [
    " - **Problem 1.2:** Write a prompt that given a famous movie returns the director."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTANEQk9bpa_"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "DIRECTOR_OF_MOVIE_PROMPT = \"\"\n",
    "\n",
    "# Grader - DO NOT CHANGE\n",
    "your_score, max_score = test_director_of_movie(DIRECTOR_OF_MOVIE_PROMPT)\n",
    "print_message(your_score, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjol744PbwIl"
   },
   "source": [
    " - **Problem 1.3:** Write a prompt that given a word, returns a list of synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guWIM6eLb2jA"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "SYNONYMS_OF_WORD_PROMPT = \"\"\n",
    "\n",
    "# Grader - DO NOT CHANGE\n",
    "your_score, max_score = test_synonyms_of_word(SYNONYMS_OF_WORD_PROMPT, first_line_only=False)\n",
    "print_message(your_score, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cuv5HzCbpzW"
   },
   "source": [
    " - **Problem 1.4:** Write a prompt that given a food item (\"cookies\"), returns a list of ingredients used to make that food item. (Hint: use `return_first_line=False` as an argument when using `run_llm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB3WOnVKbv_R"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "INGREDIENTS_OF_FOOD_PROMPT = \"\"\n",
    "\n",
    "# Grader - DO NOT CHANGE\n",
    "your_score, max_score = test_ingredients_of_food(INGREDIENTS_OF_FOOD_PROMPT, first_line_only=False)\n",
    "print_message(your_score, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "droxlfchrFiK"
   },
   "source": [
    "* **Problem 1.5:** Write a prompt that given a famous quote (\"One small step for man, one giant leap for mankind.\", quote characters included), returns the name of the person who said the quote (quotee).\n",
    "\n",
    "*Extra Challenge:* We want you to try to complete this one without question marks (\"?\") or question words (\"Who\", \"What\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZxbEh14rF3e"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "QUOTEE_OF_QUOTE_PROMPT = \"\"\n",
    "\n",
    "# Grader\n",
    "your_score, max_score = test_quotee_of_quote(QUOTEE_OF_QUOTE_PROMPT)\n",
    "print_message(your_score, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpJIgznD_mw4"
   },
   "source": [
    "# Section 2: Prompt Engineering\n",
    "\n",
    "\n",
    "\n",
    "The prompts you have used up to this point have been fairly basic and straightforward to create. But what if you have a more difficult task and it seems like your prompt isn't working? *Prompt engineering* is the procecss of iterating on a prompt in clever ways to induce the model to produce what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEkQ_gUOTPOT"
   },
   "source": [
    "A prompt that is well-engineered can effectively solve difficult NLP tasks that previously were solved by fine-tuning models.\n",
    "\n",
    "**Problem 2.1:** Write a prompt that will solve the [sentiment classification task](https://en.wikipedia.org/wiki/Sentiment_analysis), and classify [movie reviews](https://ai.stanford.edu/~amaas/data/sentiment/) as *positive* or *negative*. `IMDB_DATASET_X` and `IMDB_DATASET_Y` contain 200 reviews and sentiment labels (1 = positive, 0 = negative). Get as high of an accuracy as you can on these. Place your `MOVIE_SENTIMENT` prompt and `POSITIVE_VEBALIZERS` and `NEGATIVE_VERBALIZERS` in your report, along with your `correct` (out of 200) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FivGbeAmVOK7"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "MOVIE_SENTIMENT_PROMPT = \"\"\n",
    "\n",
    "POSITIVE_VERBALIZERS = [\n",
    "    \"good\",\n",
    "    # TODO - Add other positive verbalizers if necessary...\n",
    "]\n",
    "NEGATIVE_VERBALIZERS = [\n",
    "    \"bad\",\n",
    "    # TODO - Add other negative verbalizers if necessary...\n",
    "]\n",
    "\n",
    "def map_to_sentiment_label(llm_output):\n",
    "    for v in POSITIVE_VERBALIZERS:\n",
    "        if v.lower() in llm_output[:20].lower():\n",
    "            return 1\n",
    "    for v in NEGATIVE_VERBALIZERS:\n",
    "        if v.lower() in llm_output[:20].lower():\n",
    "            return 0\n",
    "    return None\n",
    "\n",
    "correct = 0\n",
    "for review, label in zip(IMDB_DATASET_X, IMDB_DATASET_Y):\n",
    "    llm_output = run_llm(MOVIE_SENTIMENT_PROMPT.replace(\"{input}\", review), return_first_line=True)\n",
    "    prediction = map_to_sentiment_label(llm_output)\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "    print(f\"Prediction: {prediction}, Label: {label}\")\n",
    "print(f\"Correct: {correct}/200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsi2i4y64kvP"
   },
   "source": [
    "# Section 3: Few-Shot Prompting\n",
    "\n",
    "The prompts you have seen up until this point are zero-shot prompts, in that we are asking the model to complete a task without any examples. By providing some examples in the prompt, the model can become more capable. For example, the following zero-shot and few-shot prompts solicit more complex words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZs-_4-S_vww",
    "outputId": "d4e1ae75-2624-40c9-8bfa-90472dd40cd9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ZERO_SHOT_COMPLEX_PROMPT = \"Question: What is a more complex word for {input}? Answer:\"\n",
    "FEW_SHOT_COMPLEX_PROMPT = \"angry : aggrieved\\nsad : depressed\\n{input} :\"\n",
    "\n",
    "print(run_llm(ZERO_SHOT_COMPLEX_PROMPT.replace(\"{input}\", 'happy')))\n",
    "print()\n",
    "print(run_llm(FEW_SHOT_COMPLEX_PROMPT.replace(\"{input}\", 'happy'), return_first_line=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yotgY6JWHCRL"
   },
   "source": [
    "Now that you've seen an example of few-shot prompting, it's your turn to try it.\n",
    "\n",
    "**Problem 3.1:** Write a few-shot prompt that translates a Korean word to an English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEf2PB9WHSus"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "KOREAN_TO_ENGLISH_PROMPT = \"\" # Solution\n",
    "\n",
    "# Grader\n",
    "your_score, max_score = test_korean_to_english(KOREAN_TO_ENGLISH_PROMPT)\n",
    "print_message(your_score, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afOSMXJ7U-ty"
   },
   "source": [
    "**Problem 3.2:** Write a few-shot prompt that converts an input into a [Jeopardy! style answer](https://en.wikipedia.org/wiki/Jeopardy!#:~:text=Rather%20than%20being%20given%20questions,the%20form%20of%20a%20question.) (The Great Lakes -> \"What are the Great Lakes?\" or Taylor Swift -> \"Who is Taylor Swift?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPxTwMR8V92B"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "TO_JEOPARDY_ANSWER_PROMPT = \"\" # Solution\n",
    "\n",
    "# Grader\n",
    "your_score, max_score = test_to_jeopardy_answer(TO_JEOPARDY_ANSWER_PROMPT)\n",
    "print_message(your_score, max_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CHrdyiQdBt2K",
    "KKvfbQtibDXM"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
