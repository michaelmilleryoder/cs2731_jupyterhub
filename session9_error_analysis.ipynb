{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dc1cc6-10e3-43e0-9b2a-9c3ab64ca08d",
   "metadata": {},
   "source": [
    "# Error analysis of clickbait classification with logistic regression\n",
    "You must be so sick of clickbait by now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc00fe5-945f-4d1a-8089-8fbd3e945426",
   "metadata": {},
   "source": [
    "## Load clickbait data from Kaggle\n",
    "This data consists of headlines classified as clickbait or not (regular news). Source site: https://www.kaggle.com/datasets/amananandrai/clickbait-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90300de0-f52f-4122-a9bf-551271e1ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset with pandas\n",
    "# 0 corresponds to not clickbait, 1 has been judged as clickbait\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set pandas to display entire texts in dataframes\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "data = pd.read_csv('data/clickbait_data.csv')\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8cc3e-5f01-448c-ad24-4522945d4f8a",
   "metadata": {},
   "source": [
    "## Split into training, development, and test sets\n",
    "It's best to do error analysis on a development set instead of a test set since you don't want to look at examples in the test set, change your model and overfit to that test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adbeff-1ebf-47b4-a88a-3711794e9681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = int(0.1 * len(data))\n",
    "rest, test = train_test_split(data, test_size=test_size, random_state=9) # split data into test and the \"rest\" (which will be train + dev)\n",
    "train, dev = train_test_split(rest, test_size=test_size, random_state=9) # split the \"rest\" into train and dev\n",
    "\n",
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809d550-dfb8-4182-b288-ae65e56de1d6",
   "metadata": {},
   "source": [
    "## Extract unigram features from the text data\n",
    "As a reminder, this step converts each headline to a numeric vector of unigram counts (how many times each word type occurs).\n",
    "\"Training\" the vectorizer means finding how many unique features (in this case, unique words) are in the training set. This sets the number of columns in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f76f8e-26ca-48b4-9a76-21f728f99a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "unigram_vectorizer.fit(train['headline']) # input is a list of strings (documents)\n",
    "train_features = unigram_vectorizer.transform(train['headline'])\n",
    "dev_features = unigram_vectorizer.transform(dev['headline'])\n",
    "test_features = unigram_vectorizer.transform(test['headline'])\n",
    "\n",
    "print(type(train_features))\n",
    "print(train_features.shape) # (number of rows in the matrix, number of columns)\n",
    "print(dev_features.shape)  # (number of rows in the matrix, number of columns)\n",
    "print(test_features.shape)  # (number of rows in the matrix, number of columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03e681-2b27-456e-b6e6-d4478944b063",
   "metadata": {},
   "source": [
    "## Train and evaluate a logistic regression model for clickbait classification\n",
    "We'll use `scikit-learn`'s `LogisticRegression` class to train a classifiers using unigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82b845-2adc-4606-8736-b68f3f60c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_unigrams = LogisticRegression() # Instantiate a logistic regression classifier\n",
    "clf_unigrams.fit(train_features, train['clickbait']) # Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dec200-f35c-42a5-bfff-c04591f35b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate unigram logistic regression classifier\n",
    "from sklearn.metrics import classification_report # this provides a bunch of useful evaluation metrics\n",
    "\n",
    "dev_labels = dev['clickbait'] # true (gold) test set labels for clickbait/not clickbait\n",
    "unigram_dev_predictions = clf_unigrams.predict(dev_features)\n",
    "\n",
    "results = pd.DataFrame(classification_report(dev_labels, unigram_dev_predictions, output_dict=True))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89042b66-a1d2-4924-81c9-56e774bf3f46",
   "metadata": {},
   "source": [
    "# Error analysis\n",
    "That's pretty good performance! But what's with those pesky errors? Let's dig into what kind of errors the system made.\n",
    "\n",
    "First we'll create what's called a **confusion matrix** or **contingency table** of different types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb1df9-5cc9-4489-beec-1698f656e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = dev_labels # the name of the variable where the test set actual labels is stored\n",
    "y_pred = unigram_dev_predictions # the name of the variable where the test set actual labels is stored\n",
    "confusion = pd.DataFrame(confusion_matrix(y_true, y_pred), columns=['pred_0', 'pred_1'], index=['true_0', 'true_1'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1669f2-100f-4749-a9bc-3c564ed77606",
   "metadata": {},
   "source": [
    "It looks like our most common error is where we predict 0 (not clickbait) and the actual label is 1 (clickbait). **What is the name of that kind of error?**\n",
    "\n",
    "Let's look at some examples of these types of errors. **Note that this should only be done with a development set, not a test set, otherwise any changes you make to the model to address errors may lead to overestimated performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1388369-cf5a-4d1c-9055-ff29944114ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['prediction'] = unigram_dev_predictions # add a column for the system predictions\n",
    "false_negatives = dev[(dev.prediction == 0) & (dev.clickbait == 1)]\n",
    "false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223469d1-c164-448d-aaae-f11ca0efe2ca",
   "metadata": {},
   "source": [
    "Do you notice any patterns in what the system might not be picking up on? It's okay to speculate here, but remember what features it's using: only unigrams (individual words).\n",
    "\n",
    "Sometimes it's useful to compare with examples the model got right (true positives in this case). Let's take a look at those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba54c1-a6e5-4b5c-8844-48af4273f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = dev[(dev.prediction == 1) & (dev.clickbait == 1)]\n",
    "true_positives.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228c75-7c76-49bf-99d3-8ead2ce5678b",
   "metadata": {},
   "source": [
    "Finally, take a look at the other type of error our system makes: false positives. **What does the system predict and what is the true label in this case?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd159d1d-eff8-410d-af0f-e8fdf70f16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = dev[(dev.prediction == 1) & (dev.clickbait == 0)]\n",
    "false_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e972e5-703c-4f01-81ec-8f48b0ce6d53",
   "metadata": {},
   "source": [
    "Do you observe any potential patterns in these false positives? It's again good to keep in mind what features the system sees: unigrams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
