{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42751b82-f077-466d-9895-a877c4f94e2e",
   "metadata": {},
   "source": [
    "# Train word2vec models and explore word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ddf3b-0a53-46c0-ae28-07da798aea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you can import gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eed1de-0079-4711-9fc7-f95449cdc8c4",
   "metadata": {},
   "source": [
    "## Load pretrained word embeddings from Google News\n",
    "These are some famous pretrained word embeddings from Google News a long time ago (like 2013). Source site: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded67e5b-2c3b-405c-8ead-0584084b7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings (takes a few minutes)\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "file_path = '/ix/cs2731_2025f/mmyoder/GoogleNews-vectors-negative300.bin' # is 3.4G, so I stored in our class storage on CRCD\n",
    "google_news_model = KeyedVectors.load_word2vec_format(file_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef7be1-097a-4d3c-96c4-1881606ff5d6",
   "metadata": {},
   "source": [
    "## Explore nearest neighbors to individual words in this Google News word embedding space\n",
    "Gensim's `most_similar` function provides the nearest neighbor words in a word embedding space to other words. That is, it finds which words have the largest cosine similarities to a word you provide. Let's check it out!\n",
    "\n",
    "Fill in a random word below. You can also provide a list of words and Gensim will search for words that are closest to the mean word embedding of those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a4cc7-763f-40a2-b5a4-f3d3ecbbac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell (or duplicate it) as many times as you want with different words or word Lists\n",
    "example_word = # FILL IN any word here, in quotes so Python knows it's a string.\n",
    "# Examples could be 'value', 'gold', 'platinum', 'reserves', 'silver', 'metals', 'copper', 'belgium', 'australia', 'china', 'grammes', \"mine\"\n",
    "google_news_model.most_similar(example_word, topn=10, restrict_vocab=50000) # compares with most frequent 50k words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de6464-82d3-4ffe-b2a0-a412e8dacdbe",
   "metadata": {},
   "source": [
    "## Visualize word embedding spaces\n",
    "We'll use a popular technique called t-SNE to reduce the dimensionality of word vectors from 300 to 2 so we can visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc58477-1449-42f2-828f-ffc2dbc032cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "\n",
    "def tsne_plot(model, words=None, num_words_to_consider=10000, num_words_to_sample=100):\n",
    "    \"\"\" Creates and TSNE model and plots it \n",
    "    Based on https://www.kaggle.com/code/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "    Args:\n",
    "        model: Word2Vec model or KeyedVectors model from gensim\n",
    "        words: A list of words to plot. If None, will plot num_words_to_sample random words. Words must be in top num_words_to_consider in the vocabulary\n",
    "        num_words_to_consider: the number of words to consider in the vocabulary, assumed to be sorted descending by frequency\n",
    "        num_words_to_sample: if words is set to None, the number of words to randomly sample to graph from the top num_words_to_consider in the vocabulary\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "\n",
    "    # Get embeddings for a random sample of words in the vocabulary\n",
    "    sample = random.sample(model.index_to_key[:num_words_to_consider], num_words_to_sample)\n",
    "    for word in sample:\n",
    "        embeddings.append(model[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # Add in custom words if present\n",
    "    if words is not None:\n",
    "        for word in words:\n",
    "            embeddings.append(model[word])\n",
    "            labels.append(word)\n",
    "\n",
    "    # Do t-SNE dimensionality reduction\n",
    "    tsne_model = TSNE(n_components=2, init='pca', max_iter=1000, random_state=9)\n",
    "    tsne_values = tsne_model.fit_transform(np.array(embeddings))\n",
    "\n",
    "    # Plot words\n",
    "    if words is not None:\n",
    "        words_to_plot = words\n",
    "        inds_to_plot = list(range(len(tsne_values)))[-1 * len(words):] # get the last set of indexes since custom words were added last\n",
    "    else:\n",
    "        words_to_plot = sample\n",
    "        inds_to_plot = list(range(len(tsne_values)))\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    plt.figure(figsize=(12, 12)) \n",
    "\n",
    "    for i in inds_to_plot:\n",
    "        value = tsne_values[i]\n",
    "        # x.append(value[0])\n",
    "        # y.append(value[1])\n",
    "        plt.scatter(value[0],value[1])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(value[0], value[1]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ef036-7488-4edd-91a8-9300196dd703",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(google_news_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6a80a-edfc-4458-9388-75edc1632f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words =  # FILL IN a list of words, some related and some not\n",
    "# For example: ['value', 'gold', 'platinum', 'reserves', 'silver', 'metals', 'copper', 'belgium', 'australia', 'china', 'grammes', 'mine']\n",
    "tsne_plot(google_news_model, words=example_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
