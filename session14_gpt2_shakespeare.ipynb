{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681ab686-82c9-4c32-8f5d-7e343d2d09d9",
   "metadata": {},
   "source": [
    "# Fine-tune GPT-2 on Shakespeare\n",
    "In this notebook, you will fine-tune GPT-2 to generate text and that sounds like Shakespeare. You will use the Hugging Face framework.\n",
    "\n",
    "References:\n",
    "* https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface\n",
    "* https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb#scrollTo=m9lHS0mIMak4\n",
    "* https://huggingface.co/docs/transformers/en/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976afdf-d3bc-44e7-a217-8527839e4691",
   "metadata": {},
   "source": [
    "## Import required packages\n",
    "And check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28332155-ea84-4768-8e88-f837a2625090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.__path__)\n",
    "print(torch.cuda.is_available()) # Check for GPU availability if installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b616921-594e-43ab-8920-a75ee8d43f1b",
   "metadata": {},
   "source": [
    "# Load Shakespeare data for training\n",
    "We'll put the data in a Hugging Face `TextDataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edd47d-d9a0-475b-90f3-639895dab076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d376527-04c4-4a99-bad0-3dd19262d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='data/shakespeare_train.txt',\n",
    "    block_size=64)\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='data/shakespeare_test.txt',\n",
    "    block_size=64)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2276d9c-c760-4b5a-ad30-7eb969251dc0",
   "metadata": {},
   "source": [
    "# Initialize training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb86495-8b6a-47c6-8d17-d1bd3dacab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-shakespeare\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 300, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6c879-a3eb-482a-a453-2dfa04c597cd",
   "metadata": {},
   "source": [
    "# Finetune (train) the model\n",
    "And save it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595688e-61b0-44ff-83d1-3d49dcdf4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20f9e4-dcfc-4935-8795-f2a0d14b4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model() # this will save to the directory specified in the TrainingArguments object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bf2b7-c005-4f5b-9fd1-e2018f53b1c8",
   "metadata": {},
   "source": [
    "# Generate text from the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8711e-03f4-48d0-b9f2-96b701295d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "shakespeare_gpt2 = pipeline('text-generation', model='./gpt2-shakespeare', tokenizer='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6773b99-df6c-4cbe-9965-fa98cce35576",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '' # FILL IN a word or two. This acts as the context (prompt) for generation.\n",
    "\n",
    "print(shakespeare_gpt2(prefix)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
