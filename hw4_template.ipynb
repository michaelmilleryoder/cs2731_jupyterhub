{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-bsGmLEA1sN"
   },
   "source": [
    "# Fine-tune BERT-based models from Hugging Face on POS-tagging for English and Norwegian\n",
    "\n",
    "This notebook will guide you through Part 2 of [CS 2731 Homework 4](https://michaelmilleryoder.github.io/cs2731_fall2024/hw4).\n",
    "\n",
    "Please copy this notebook and name it `{pitt email id}_hw4_bert_pos.ipynb`.\n",
    "\n",
    "Code for loading and preprocessing the data is provided. You will provide code for training and evaluation using Hugging Face Trainer or PyTorch.\n",
    "\n",
    "Run all the cells starting from the top, filling in any sections that need to be filled in. Spots you need to fill in are specified.\n",
    "\n",
    "You will want to duplicate cells in each section for each language (English or Norwegian) or create separate sections in the notebook for separate languages.\n",
    "\n",
    "**Note**: Please run this notebook on a GPU server on the CRCD.\n",
    "\n",
    "The tutorials below from Hugging Face are informative. You can use code from them and adapt to this use case.\n",
    "* [Token classification (sequence labeling) with Hugging Face](https://huggingface.co/docs/transformers/en/tasks/token_classification)\n",
    "* [Hugging Face `Trainer` class tutorial](https://huggingface.co/docs/transformers/en/training#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn2M1fXE1obQ"
   },
   "source": [
    "# Load data\n",
    "\n",
    "Here you will be loading the training, dev, and test datasets of English and Norwegian text annotated with POS tags. The data are from the [Universal Dependencies](https://universaldependencies.org/) project.\n",
    "\n",
    "We will be using the universal part-of-speech tags in the `upos` column.\n",
    "\n",
    "Note:  There are 2 written forms of Norwegian: Bokmål and Nynorsk: https://en.wikipedia.org/wiki/Norwegian_language. This data is in the Bokmål written form.\n",
    "\n",
    "Here is a link to learn more about the data:\n",
    "* [Universal Dependencies data format](https://universaldependencies.org/format.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "en_data = load_from_disk('data/en_ewt_hf')\n",
    "print(en_data)\n",
    "no_data = load_from_disk('data/no_bokmaal_hf')\n",
    "print(no_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wutZKsou-9zK"
   },
   "outputs": [],
   "source": [
    "# Take a look at the part of speech tags\n",
    "\n",
    "tags = en_data['train'].features['upos_labels'].feature.names\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count and explore POS tags in this dataset\n",
    "For both English and Norwegian datasets, calculate the following and put in your report:\n",
    "\n",
    "* The 5 most frequent POS tags and how many tokens are tagged with each\n",
    "* For each of the 5 most frequent POS tags, find the 5 most frequent word types annotated with that tag in the training data\n",
    "\n",
    "Be sure to report the actual POS tag names in your report, not the IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM4wjjncC8P6"
   },
   "source": [
    "# Tokenization\n",
    "Fill in code in this section to prepare the input with subword tokenization for BERT. You can follow the process in the [Hugging Face token classification guide](https://huggingface.co/docs/transformers/en/tasks/token_classification).\n",
    "\n",
    "Here is also where you will decide on which BERT-based pre-trained model you will fine-tune, since you will need to match its tokenization.\n",
    "Feel free to search Hugging Face for BERT variants or to use recommended ones in Hugging Face documentation. For Norwegian, you'll want a pretrained BERT model that can handle Norwegian (in Bokmål written form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hr5GJU3TC3o-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# FILL IN with the name of a BERT-based pretrained model from Hugging Face\n",
    "pretrained_model =\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAEdGRdmaf3K"
   },
   "source": [
    "Subword tokenization will add special tokens such as `[CLS]` which we want the classifier to ignore.\n",
    "\n",
    "It also splits some words into multiple tokens. We'll have to re-align those to assign just one part-of-speech tag to each word.\n",
    "\n",
    "Fill in code here to do this alignment, as well as prepare a tokenized version of the dataset. You may adapt code from the [Hugging Face token classification guide](https://huggingface.co/docs/transformers/en/tasks/token_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dx7btw5lEla2"
   },
   "outputs": [],
   "source": [
    "# FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrzZ7suyFiJ1"
   },
   "source": [
    "# Prepare evaluation\n",
    "\n",
    "Evaluation code is provided here.\n",
    "\n",
    "Source: [Hugging Face token classification guide](https://huggingface.co/docs/transformers/en/tasks/token_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f0e1a11407e14799b4c6d91babd7ba84",
      "7a83fd282c7a42a3abbbeb1be3b5fd8f",
      "8177dd0358bd46d58af7ce9035023abb",
      "fe91203e3a6c413b84fcd025f69f761c",
      "0814697e467b4476b76a2d90cb702416",
      "d8649ef4e46d4c938abb625516aacd08",
      "6bacee2b2b2b4cb1b416f3cb81a83fd0",
      "1f6bcb193d6e4795abc7bf9644cb267e",
      "6dbc1fd76cf64a1faba282cabe9ba1aa",
      "2eb403a2c395445f888c9c37d0bd2ee4",
      "3c31f3dc569f49a699c6d5b1e0b087ac"
     ]
    },
    "executionInfo": {
     "elapsed": 15147,
     "status": "ok",
     "timestamp": 1729819531082,
     "user": {
      "displayName": "Michael Yoder",
      "userId": "01658005467682064388"
     },
     "user_tz": 240
    },
    "id": "I48I5wJJFui0",
    "outputId": "18db863e-5038-4fa9-f811-3a44c26dc496"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1729819531085,
     "user": {
      "displayName": "Michael Yoder",
      "userId": "01658005467682064388"
     },
     "user_tz": 240
    },
    "id": "PbAcHDJbF4a9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_list = dataset['train'].features['upos_labels'].feature.names\n",
    "labels = dataset['train'][0]['upos_labels']\n",
    "labels = [label_list[i] for i in labels]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey-86X_EGQCK"
   },
   "source": [
    "# Train (fine-tune) the model\n",
    "\n",
    "Fill in code here to load your pretrained model and do fine-tuning using the `Trainer` class or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDyIyfQtGSmL"
   },
   "outputs": [],
   "source": [
    "# FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-6114reLoKZ"
   },
   "source": [
    "# Test performance\n",
    "\n",
    "Fill in code here to evaluate your fine-tuned model's performance on the test set of the tokenized dataset.\n",
    "\n",
    "You will be reporting accuracy in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWaWR04hLerC"
   },
   "outputs": [],
   "source": [
    "# FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2pp8gF7F410"
   },
   "source": [
    "# Run on an example sentence\n",
    "\n",
    "Fill in code here to run your classifier on two example sentences choice for both English and Norwegian models:\n",
    "1. The provided example sentence\n",
    "2. An example sentence of your choice (feel free to use a translation service like Google Translate if you don't know Norwegian)\n",
    "\n",
    "You will likely have to load these models from checkpoints created during training.\n",
    "\n",
    "Provide the predicted tags for example sentences in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9L6wbiBJMhI"
   },
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "en_example = \"Hello, I am a student at the University of Pittsburgh.\"\n",
    "no_example = \"Hallo, jeg er student ved University of Pittsburgh.\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
