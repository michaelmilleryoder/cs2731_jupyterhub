{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca1a81b-d232-4673-b132-b75fb8f6d1fb",
   "metadata": {},
   "source": [
    "# Import necessary packages\n",
    "Test out the environment to make sure you have the packages needed to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54de36-d46d-405c-9484-06638e22d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test importing package/s.ðŸ¤ž for no errors!\n",
    "import pandas\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e0404-e15e-442f-a96d-9db75c80eb58",
   "metadata": {},
   "source": [
    "If you get any errors, please run the following cell to display your PATH environment variable. If you don't have any errors, no need to run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576136f-f303-4dcf-bb08-58c204e6f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f5ed9-ca44-405d-bfe8-b298e3d341ca",
   "metadata": {},
   "source": [
    "# Download Airbnb data\n",
    "We will be using data from public Airbnb listings for this class session. This data has been collected by [Inside Airbnb](https://insideairbnb.com/) and was used for an award-winning NLP paper ([Brunila et al. 2023](https://aclanthology.org/2023.emnlp-main.284/) if you're curious.)\n",
    "\n",
    "To download the data:\n",
    "1. Go to https://insideairbnb.com/get-the-data/\n",
    "2. Choose a city's data under **Data Downloads**.\n",
    "3. Right-click `listings.csv.gz` and copy the link (URL).\n",
    "4. Paste it as `listings_url` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a65933-f8e3-4ba1-a1a8-46bcf13b9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_url = '' # FILL IN with your URL\n",
    "output_filename = '' # FILL IN a name for your output file\n",
    "\n",
    "# Download and unzip data\n",
    "! wget {listings_url} -O {output_filename}.csv.gz \n",
    "! gunzip -c {output_filename}.csv.gz > {output_filename}.csv # This unzips the file into a regular CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abb301-2a98-489a-86ac-ec4b985c33a0",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08891f04-fe88-47a8-9866-510600b34c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "listings = pd.read_csv(f'{output_filename}.csv') # reads CSV file into a pandas dataframe\n",
    "listings.info() # provide basic information about this dataframe\n",
    "listings.head() # see first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293743a-ffbe-4c05-a426-336922c36290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand pandas view (good for seeing more of text)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "listings[['description']].head() # Look at the description text column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658d329-3795-409b-90c9-393d4e2d8697",
   "metadata": {},
   "source": [
    "# Cleaning with regular expressions\n",
    "Remove any extraneous text with regular expression pattern matching.\n",
    "We will use [pandas' built-in functions for processing strings](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#string-methods) to do this. These functions apply to the string transformation to each element in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd6231-04ef-4730-a13a-b8303ff7d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_to_replace = r'' # FILL IN regular expression here\n",
    "replace_with = ' ' # potentially fill in with what you want it to be replaced with\n",
    "\n",
    "listings['description_processed'] = listings['description'].str.replace(pattern_to_replace, replace_with, regex=True)\n",
    "listings[['description', 'description_processed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91726cc3-f784-4298-9ee0-52a39d2e2fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NaN values to empty strings\n",
    "listings['description_processed'] = listings['description_processed'].fillna('')\n",
    "listings[['description', 'description_processed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7933202-f83e-4190-8422-1112f126fc66",
   "metadata": {},
   "source": [
    "# Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146442e8-b1bb-466c-abc3-1fe0f953763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings['description_processed'] = listings['description_processed'] # FILL IN pandas string function to lowercase\n",
    "listings[['description', 'description_processed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2975e7-0a5b-44fe-ba78-f5b017036224",
   "metadata": {},
   "source": [
    "# Prepare to remove stopwords and punctuation\n",
    "Stopwords are common \"function words\" that serve to connect other words and don't provide much new information. Examples are \"to\", \"and\", and \"of\".\n",
    "\n",
    "We will start with a list from the `nltk` (Natural Language Toolkit) package and add punctuation, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb423482-3c4a-4799-beed-88683502e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # only need to do once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d0e94-f711-4ed3-a2d5-07181ace7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "print(f'NLTK stopwords: {stops}')\n",
    "print(len(stops))\n",
    "print()\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "print(punctuation)\n",
    "print()\n",
    "\n",
    "stops += punctuation\n",
    "len(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b15583-e75b-4f74-9d38-6198aef82044",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking text up into words! Here we will use the `nltk` package to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06400aca-37a6-4649-a5f9-d647b0ed6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to do once\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9f171-e85f-4332-9c7b-1195efc2f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar since it could take awhile\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply tokenizer from nltk to column\n",
    "# Also remove any tokens that are stopwords\n",
    "def tokenize(text):\n",
    "    tokens_list = nltk.word_tokenize(text)\n",
    "    tokens_list_no_stops = [tok for tok in tokens_list if not tok in stops]\n",
    "    return ' '.join(tokens_list_no_stops)\n",
    "    \n",
    "listings['description_processed'] = listings['description_processed'].progress_map(tokenize)\n",
    "listings[['description', 'description_processed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964dbc4-6e31-49ab-a402-d71eeae95066",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09135c38-0fbb-444f-87cb-ef8fd749a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar since it takes awhile\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    tokens = text.split()\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "listings['description_processed'] = listings['description_processed'].progress_map(stem)\n",
    "listings[['description', 'description_processed']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
